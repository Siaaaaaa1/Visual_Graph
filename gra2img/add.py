from datasets import load_dataset

# ======================
# è·¯å¾„é…ç½®
# ======================
INPUT_PARQUET = "./pubmed_train_with_prompt.parquet"
OUTPUT_PARQUET = "./pubmed_train_with_all.parquet"

# ======================
# å ä½ promptï¼ˆåªç”¨äºéª—è¿‡ RLDatasetï¼‰
# âš ï¸ ä¸ä¼šè¢«çœŸæ­£é€è¿› VLM
# ======================
PLACEHOLDER_PROMPT = [
    {
        "role": "user",
        "content": "<image>\nYou are an agent."
    }
]

# ======================
# data_sourceï¼ˆç»™ trainer / protocol ç”¨ï¼‰
# ======================
DATA_SOURCE = "graph_search"


def process_example(example):
    """
    å¯¹æ¯ä¸€è¡Œæ ·æœ¬åšæœ€å°ã€å¿…è¦çš„ä¿®æ­£
    """

    # 1ï¸âƒ£ åˆ æ‰æ— ç”¨å­—æ®µï¼ˆé¿å…æ­§ä¹‰ï¼‰
    example.pop("split", None)

    # 2ï¸âƒ£ è¡¥ promptï¼ˆRLDataset å¿…é¡»ï¼‰
    if "prompt" not in example or example["prompt"] is None:
        example["prompt"] = PLACEHOLDER_PROMPT

    # 3ï¸âƒ£ è¡¥ data_sourceï¼ˆTrainer / protocol å¿…é¡»ï¼‰
    if "data_source" not in example or example["data_source"] is None:
        example["data_source"] = DATA_SOURCE

    return example


def main():
    # è¯»å– parquetï¼ˆå½“æˆä¸€ä¸ª Datasetï¼Œç”¨ train split å³å¯ï¼‰
    dataset = load_dataset(
        "parquet",
        data_files=INPUT_PARQUET,
        split="train"
    )

    # å¤„ç†å­—æ®µ
    dataset = dataset.map(
        process_example,
        desc="Cleaning fields & adding prompt/data_source"
    )

    # ä¿å­˜æ–° parquet
    dataset.to_parquet(OUTPUT_PARQUET)
    print(f"âœ… Saved cleaned parquet to: {OUTPUT_PARQUET}")

    # å¯é€‰ï¼šæ‰“å°ä¸€æ¡æ ·æœ¬ç¡®è®¤
    print("\nğŸ” Sample row after processing:")
    sample = dataset[0]
    for k in sample.keys():
        if k in ("image_bytes",):
            print(f"{k}: <bytes>")
        else:
            print(f"{k}: {sample[k]}")


if __name__ == "__main__":
    main()
