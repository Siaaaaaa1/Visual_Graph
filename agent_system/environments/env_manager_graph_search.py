import re
from typing import Any, Dict, List, Tuple

from agent_system.environments.base import EnvironmentManagerBase, to_numpy
from agent_system.memory import FullSequenceSearchMemory

# =================================================================
# 1. 固定任务指令 (Task Instruction)
# =================================================================
GRAPH_SEARCH_TASK_INSTRUCTION = """
# GRAPH SEARCH TASK INSTRUCTION

You are a graph reasoning agent. Your task is to predict the correct semantic category of a provided center node.

---

## 1. Initial State & Context
* **Input:** You are provided with a **center node** (ID and text), its statistical metadata, and a visualized **subgraph** (up to 20 nodes).
* **History & Updates:** From step 2 onwards, you will additionally see the summaries and actions from previous steps, observations from the most recent steps, and the graph visualization updated according to your previous requests.

## 2. Reasoning Guidelines
Your reasoning must be **holistic** and integrate multiple dimensions:
* **Textual Content:** The raw text of the center node and its neighbors.
* **Graph Topology:** Distribution and connectivity of categories.
* **Neighboring Node Information:** The text, predicted categories, and statistical context of surrounding nodes.
The Center Node may relate to multiple categories; therefore, you must carefully synthesize the Center Node's text, graph topology, and neighboring node information to make a final judgment. There is only one correct final answer.

## 3. Objective & Accuracy
* **Goal:** Submit the correct category. Rewards are strictly based on **accuracy**. Minimize exploration steps while maintaining accuracy. You must submit the final answer within 10 steps; otherwise, the session will be forcibly terminated.
* **Verification First:** You are a fact-verification agent. Do not guess. 
* **Grounding:** If the information is insufficient, it is recommended to use the `check_node` action to read the actual text content of nodes you deem **critical** (e.g., nodes close in vector or topological distance to the center node, or nodes inconsistent with surrounding predicted categories that affect your judgment) before making a final prediction. If the current graph `view_mode` is too broad or too narrow to support a decision, use `check_graph` to adjust the view (e.g., retrieving 1-hop/2-hop neighbors or semantically similar nodes).

## 4. ⚠️ Critical Warning
The neighbor labels are generated by 'qwen-plus' and are **not guaranteed to be accurate**. 
* Treat them as **references only**, the prediction accuracy of 'qwen-plus' is approximately 80%–90%.
* You must rely on your own inspection of the raw text to determine true semantic relationships.

If the current graph lacks sufficient information for a correct judgment, utilize interaction actions to gather more data. For example, if current node categories appear chaotic, use `check_graph` to expand your scope (increasing the node limit or changing the `view_mode`). You may also inspect nearby nodes of varying categories to verify their labels and contribution to the center node's context. Continue this process until you have gathered sufficient information to make a confident final decision.

---

## 5. Interaction Actions
At each step, you must choose **exactly one** of the following actions:

### A. Inspect Node Text
Read the actual content of specific nodes; you may check up to 5 nodes at a time.
* **Command:** `<action>check_node:ID</action>` or `<action>check_nodes:[ID1, ID2, ...]</action>`

### B. Update Graph Visualization
Render a subgraph centered on the target node. In the visualization: hollow circles represent nodes (colored by 'qwen-plus' predicted categories); the central black node is the target Center Node; numbers inside circles denote Node IDs; and lines represent edges. You may update the view multiple times.
* **Command:** `<action>check_graph:view_mode,max_nodes</action>`
* **Valid View Modes:**
    * `1-hop`: Immediate neighbors only.
    * `2-hop`: 1-hop and 2-hop neighbors.
    * `sim`: Highest semantic similarity nodes; closer distance to the center node in the image indicates higher semantic similarity.
    * `1-hop+sim`: 1-hop neighbors + high-similarity nodes.
    * `2-hop+sim`: 1&2-hop neighbors + high-similarity nodes.
* **Max Nodes:** Integer limit (e.g., 10, 20) representing the maximum number of nodes displayed; keeping it under 50 is recommended.

### C. Submit Answer
Submit the final category when confident.
* **Command:** `<action>final:Category Name</action>`
* **Note:** The name must exactly match the provided candidate list.

---

## 6. Response Format
You must strictly follow this sequence in your output:

1. **Analysis:** Analyze the current info and history. Wrap inside `<think>...</think>`.
2. **Summary:** Consolidate verified facts and critical clues. Wrap inside `<summary>...</summary>`. 
   * *Note: This serves as your memory for the next step. Keep it dense and remove redundant info. You need to include key information or reasoning results obtained in this step, or other critical details helpful for determining the center node's category, rather than the entire reasoning process.
3. **Action:** Output your chosen action on a new line wrapped in `<action>...</action>`.
"""
# GRAPH_SEARCH_TASK_INSTRUCTION = """You are a graph reasoning agent. Your task is to predict the correct semantic category of a provided center node.

# In the initial state, you are given the center node (ID and text), its statistical information, and a **2-hop subgraph visualization** (max 20 nodes). You can see the center node and its surrounding context immediately.

# As you reason, utilize the principle of graph homophily: nodes that are topologically closer to the center node (especially direct 1-hop neighbors) are statistically more likely to share the same semantic category than distant nodes. However, do not rely on topology alone.

# Your ultimate goal is to submit the correct category. Please note that you will receive a reward ONLY if your final answer is correct. Therefore, while you should aim for efficiency, you must prioritize ACCURACY over speed. You represent an agent that verifies facts: do not guess based on weak evidence. You must verify your hypothesis by reading the text content of the center node and its neighbors to ensure your prediction is grounded in data.

# A critical warning regarding neighbor categories: The categories of neighboring nodes (displayed in the graph legend or candidate lists) are inferred predictions generated by the 'qwen-plus' model. These labels are NOT guaranteed to be accurate and should be used for reference only. You must rely on your own reasoning by inspecting the actual text content of the nodes via the check_node action to determine the true semantic relationships.

# At each step, you must choose exactly one of the following three interaction actions:

# 1. Inspect Node Text: This allows you to read the content of specific nodes.
#    Command: <action>check_node:ID</action> or <action>check_nodes:[ID1, ID2, ...]</action>

# 2. Update Graph Visualization: This renders a subgraph centered on the target node. You must specify a 'View Mode' and 'Max Nodes'.
#    Command: <action>check_graph:view_mode,max_nodes</action>
#    The valid view modes are:
#    - '1-hop': Shows only immediate neighbors.
#    - '2-hop': Shows 1-hop and 2-hop neighbors.
#    - 'sim': Shows nodes with the highest semantic similarity regardless of distance.
#    - '1-hop+sim': Prioritizes 1-hop neighbors, filling the remaining count with high-similarity nodes.
#    - '2-hop+sim': Prioritizes 1-hop and 2-hop neighbors, filling the rest with high-similarity nodes.
#    Max Nodes is an integer limit (e.g., 10, 20).

# 3. Submit Answer: When you are confident, submit the final category.
#    Command: <action>final:Category Name</action>
#    (Ensure the category name exactly matches one from the provided candidate lists).

# Response Format:
# You must strictly follow this sequence in your output:
# 1. Analysis: Analyze the current information and history. Wrap this reasoning inside <think>...</think> tags.
# 2. Summary: Consolidate the key evidence gathered so far. Wrap this inside <summary>...</summary> tags. IMPORTANT: This summary will be passed as memory to the next step. You must carefully balance length and information density. Include ONLY the verified facts and critical clues that are strictly useful for subsequent reasoning, and discard redundant or irrelevant details.
# 3. Action: On a new line, output the chosen action wrapped in <action>...</action> tags.
# """

# =================================================================
# 2. Prompt 模板
# =================================================================
# 优化点 2: 调整 Image 位置，将其放在 Initial/History 之后，明确标记为 "Current View"

GRAPH_SEARCH_TEMPLATE_NO_HIS = """{task_instruction}

=== Initial State ===
Center Node Info:
{initial_state}

=== Current Observation ===
Current Visual View (Initial 2-hop State): <image>

Current step: 1

Response Format:
1. First, analyze the current initial information (center node text, stats, and graph view) to decide your first step. Wrap your reasoning inside <think>...</think> tags.
2. Next, create an initial summary inside <summary>...</summary> tags. Extract the most critical facts from the initial state that will guide your subsequent reasoning.
3. Finally, on a new line, output the chosen action wrapped in <action>...</action> tags.
"""

GRAPH_SEARCH_TEMPLATE_WITH_HIS = """{task_instruction}

=== Initial State (Reference) ===
Center Node Info:
{initial_state}

=== History ===
(The following is a log of your previous actions and observations):
{memory_context}

=== Current Observation ===
Current Visual View (Snapshot after your last action): <image>

Current step: {step_count}

Response Format:
1. First, review the history and analyze the current state (especially the updated image). Wrap your reasoning inside <think>...</think> tags.
2. Next, consolidate key evidence into a concise summary inside <summary>...</summary> tags. IMPORTANT: Balance length and information density. Retain ONLY verified facts and critical clues strictly useful for the next step, and discard redundant details.
3. Finally, on a new line, output the chosen action wrapped in <action>...</action> tags.
"""

# =========================
# 3. 管理器 (Manager)
# =========================

class GraphSearchEnvironmentManager(EnvironmentManagerBase):
    """
    图搜索环境管理器。
    负责在 Batch 模式下管理环境交互、维护搜索记忆并构建发送给模型的 Prompt 文本。
    """

    def __init__(self, envs, projection_f, config):
        """
        初始化管理器。
        :param envs: 向量化环境实例。
        :param projection_f: 动作映射函数，将文本解析为底层环境可执行的动作。
        :param config: 配置对象，需包含 history_length 等参数。
        """
        self.memory = FullSequenceSearchMemory()
        super().__init__(envs, projection_f, config)
        
        self._think_pattern = re.compile(r"<think>(.*?)</think>", re.DOTALL | re.IGNORECASE)
        self._summary_pattern = re.compile(r"<summary>(.*?)</summary>", re.DOTALL | re.IGNORECASE)
        self._action_pattern = re.compile(r"<action>(.*?)</action>", re.DOTALL | re.IGNORECASE)

    def reset(self, kwargs) -> Tuple[Dict[str, Any], List[Dict]]:
        """
        重置 Batch 环境。
        :return: 包含文本 Prompt、图像观测和中心点信息的字典。
        """
        text_obs, image_obs, infos = self.envs.reset(kwargs=kwargs)

        # 记录每个样本的初始文本状态（中心点基本信息）
        self.initial_states = text_obs
        # 初始化搜索记忆
        self.memory.reset(batch_size=len(text_obs))

        observations = {
            "text": self.build_text_obs(init=True), 
            "image": image_obs, 
            "anchor": text_obs.copy(), 
        }

        return observations, infos

    def step(self, text_actions: List[str]):
        """
        执行模型输出的动作字符串，并更新状态。
        """
        # 1. 提取 Summary 和 Think (用于 Info 和 记忆)
        summaries = []
        thinks = []
        
        for raw_text in text_actions:
            # 提取 Summary
            s_match = self._summary_pattern.search(raw_text)
            if s_match:
                summaries.append(s_match.group(1).strip())
            else:
                summaries.append("No summary provided.")
            
            # 提取 Think (仅用于记录/Info)
            t_match = self._think_pattern.search(raw_text)
            if t_match:
                thinks.append(t_match.group(1).strip())
            else:
                thinks.append(None)

        # 2. 映射动作
        actions, valids = self.projection_f(text_actions)

        # 3. 执行环境交互
        next_text_obs, next_image_obs, rewards, dones, infos = self.envs.step(actions)

        # 4. 存入记忆 (包含 Action, Observation, Summary)
        self.memory.store({
            "search": actions,
            "information": next_text_obs,
            "summary": summaries 
        })

        # 5. 构建下一步观测
        next_observations = {
            "text": self.build_text_obs(init=False), 
            "image": next_image_obs, 
            "anchor": next_text_obs.copy(),
        }

        # 6. 将解析结果和额外信息注入 infos
        for i, info in enumerate(infos):
            info["is_action_valid"] = to_numpy(valids[i])
            info["parsed_think"] = thinks[i]
            info["parsed_summary"] = summaries[i]
            a_match = self._action_pattern.search(text_actions[i])
            info["parsed_action_content"] = a_match.group(1).strip() if a_match else "No Action Found"

        return next_observations, to_numpy(rewards), to_numpy(dones), infos

    def build_text_obs(self, init: bool) -> List[str]:
        """
        组装最终的文本 Prompt。
        """
        batch_size = len(self.initial_states)
        rendered_prompts: List[str] = []

        # 获取历史上下文
        if not init:
            memory_ctx, _ = self.memory.fetch(
                self.config.env.history_length,
                obs_key="information",
                action_key="search",
                summary_key="summary"
            )
        else:
            memory_ctx = [""] * batch_size

        for i in range(batch_size):
            if init:
                prompt = GRAPH_SEARCH_TEMPLATE_NO_HIS.format(
                    task_instruction=GRAPH_SEARCH_TASK_INSTRUCTION,
                    initial_state=self.initial_states[i],
                )
            else:
                prompt = GRAPH_SEARCH_TEMPLATE_WITH_HIS.format(
                    task_instruction=GRAPH_SEARCH_TASK_INSTRUCTION,
                    initial_state=self.initial_states[i],
                    memory_context=memory_ctx[i],
                    step_count=len(self.memory[i]),
                )
            rendered_prompts.append(prompt)

        return rendered_prompts