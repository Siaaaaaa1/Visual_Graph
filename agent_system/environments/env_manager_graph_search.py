import re
from typing import Any, Dict, List, Tuple

from agent_system.environments.base import EnvironmentManagerBase, to_numpy
# 引入新的 Memory 类
from agent_system.memory import FullSequenceSearchMemory

# =================================================================
# 1. 固定任务指令 (Task Instruction)
# =================================================================

# =================================================================
# 1. 固定任务指令 (Task Instruction)
# =================================================================
"""你是一个图推理智能体。你的任务是预测所提供的**中心节点**的正确语义类别。

在初始状态下，你会获得中心节点（ID 和文本）、其统计信息以及候选类别列表。至关重要的是，你最初**无法**直接看到图结构或邻居节点；你必须通过主动查询图视图来观察它们。

在推理过程中，请利用**图同质性 (Graph Homophily)** 原则：拓扑结构上距离中心节点较近的节点（尤其是一阶邻居）在统计学上比远处节点更有可能属于同一语义类别。然而，不要仅仅依赖拓扑结构。

你的终极目标是提交正确的类别。请注意，**只有**当你的最终答案正确时，你才会获得奖励。因此，虽然你应该追求效率，但必须**将准确性置于速度之上**。你代表的是一个验证事实的智能体：切勿基于微弱的证据进行猜测。你必须通过阅读中心节点及其邻居节点的文本内容来验证你的假设，确保你的预测是有据可依的。

**关于邻居类别的关键警示：** 邻居节点的类别（显示在图例或候选列表中）是由 'qwen-plus' 模型生成的推断预测。这些标签**不保证**准确，仅供参考。你必须依靠自己的推理，通过 `check_node` 操作检查节点的实际文本内容，以确定真实的语义关系。

在每一步中，你必须从以下三种交互操作中选择其一：

1. **检查节点文本 (Inspect Node Text)：** 允许你阅读特定节点的内容。
命令：`<action>check_node:ID</action>` 或 `<action>check_nodes:[ID1, ID2, ...]</action>`
2. **更新图可视化 (Update Graph Visualization)：** 渲染一个以目标节点为中心的子图。你必须指定“视图模式 (View Mode)”和“最大节点数 (Max Nodes)”。
命令：`<action>check_graph:view_mode,max_nodes</action>`
有效的视图模式包括：
* `'1-hop'`: 仅显示直接邻居。
* `'2-hop'`: 显示一阶和二阶邻居。
* `'sim'`: 无论距离远近，显示语义相似度最高的节点。
* `'1-hop+sim'`: 优先显示一阶邻居，剩余配额由高相似度节点填充。
* `'2-hop+sim'`: 优先显示一阶和二阶邻居，其余由高相似度节点填充。
最大节点数是一个整数限制（例如：10, 20）。


3. **提交答案 (Submit Answer)：** 当你有把握时，提交最终类别。
命令：`<action>final:类别名称</action>`
（确保类别名称与提供的候选列表中的名称完全一致）。

**响应格式：**
你在输出中必须严格遵守以下顺序：

1. **分析 (Analysis)：** 分析当前信息和历史记录。将此推理过程封装在 `<think>...</think>` 标签内。
2. **总结 (Summary)：** 巩固目前收集到的关键证据。将其封装在 `<summary>...</summary>` 标签内。**重要提示：** 此总结将作为“记忆”传递到下一步。你必须仔细平衡长度和信息密度。**仅**包含对后续推理有绝对作用的已验证事实和关键线索，丢弃冗余或无关的细节。
3. **操作 (Action)：** 在新的一行，输出封装在 `<action>...</action>` 标签内的所选操作。
"""


GRAPH_SEARCH_TASK_INSTRUCTION = """You are a graph reasoning agent. Your task is to predict the correct semantic category of a provided center node.

In the initial state, you are given the center node (ID and text), its statistical information, and lists of candidate categories. Crucially, you initially CANNOT see the graph structure or neighbors directly; you must actively query the graph view to observe them.

As you reason, utilize the principle of graph homophily: nodes that are topologically closer to the center node (especially direct 1-hop neighbors) are statistically more likely to share the same semantic category than distant nodes. However, do not rely on topology alone.

Your ultimate goal is to submit the correct category. Please note that you will receive a reward ONLY if your final answer is correct. Therefore, while you should aim for efficiency, you must prioritize ACCURACY over speed. You represent an agent that verifies facts: do not guess based on weak evidence. You must verify your hypothesis by reading the text content of the center node and its neighbors to ensure your prediction is grounded in data.

A critical warning regarding neighbor categories: The categories of neighboring nodes (displayed in the graph legend or candidate lists) are inferred predictions generated by the 'qwen-plus' model. These labels are NOT guaranteed to be accurate and should be used for reference only. You must rely on your own reasoning by inspecting the actual text content of the nodes via the check_node action to determine the true semantic relationships.

At each step, you must choose exactly one of the following three interaction actions:

1. Inspect Node Text: This allows you to read the content of specific nodes.
   Command: <action>check_node:ID</action> or <action>check_nodes:[ID1, ID2, ...]</action>

2. Update Graph Visualization: This renders a subgraph centered on the target node. You must specify a 'View Mode' and 'Max Nodes'.
   Command: <action>check_graph:view_mode,max_nodes</action>
   The valid view modes are:
   - '1-hop': Shows only immediate neighbors.
   - '2-hop': Shows 1-hop and 2-hop neighbors.
   - 'sim': Shows nodes with the highest semantic similarity regardless of distance.
   - '1-hop+sim': Prioritizes 1-hop neighbors, filling the remaining count with high-similarity nodes.
   - '2-hop+sim': Prioritizes 1-hop and 2-hop neighbors, filling the rest with high-similarity nodes.
   Max Nodes is an integer limit (e.g., 10, 20).

3. Submit Answer: When you are confident, submit the final category.
   Command: <action>final:Category Name</action>
   (Ensure the category name exactly matches one from the provided candidate lists).

Response Format:
You must strictly follow this sequence in your output:
1. Analysis: Analyze the current information and history. Wrap this reasoning inside <think>...</think> tags.
2. Summary: Consolidate the key evidence gathered so far. Wrap this inside <summary>...</summary> tags. IMPORTANT: This summary will be passed as memory to the next step. You must carefully balance length and information density. Include ONLY the verified facts and critical clues that are strictly useful for subsequent reasoning, and discard redundant or irrelevant details.
3. Action: On a new line, output the chosen action wrapped in <action>...</action> tags.
"""

# =================================================================
# 2. Prompt 模板
# =================================================================

GRAPH_SEARCH_TEMPLATE_NO_HIS = """{task_instruction}

Initial state:
{initial_state}
Image: <image>

Response Format:
1. First, analyze the current initial information (center node text and stats) to decide your first step. Wrap your reasoning inside <think>...</think> tags.
2. Next, create an initial summary inside <summary>...</summary> tags. Extract the most critical facts from the initial state that will guide your subsequent reasoning.
3. Finally, on a new line, output the chosen action wrapped in <action>...</action> tags.
"""

GRAPH_SEARCH_TEMPLATE_WITH_HIS = """{task_instruction}

Initial state:
{initial_state}
Image: <image>

History (previous actions and observations):
{memory_context}

Current step: {step_count}

Response Format:
1. First, review the history and analyze the current state. Wrap your reasoning inside <think>...</think> tags.
2. Next, consolidate key evidence into a concise summary inside <summary>...</summary> tags. IMPORTANT: Balance length and information density. Retain ONLY verified facts and critical clues strictly useful for the next step, and discard redundant details.
3. Finally, on a new line, output the chosen action wrapped in <action>...</action> tags.
"""

# =========================
# 3. 管理器 (Manager)
# =========================

class GraphSearchEnvironmentManager(EnvironmentManagerBase):
    """
    图搜索环境管理器。
    负责在 Batch 模式下管理环境交互、维护搜索记忆并构建发送给模型的 Prompt 文本。
    """

    def __init__(self, envs, projection_f, config):
        """
        初始化管理器。
        :param envs: 向量化环境实例。
        :param projection_f: 动作映射函数，将文本解析为底层环境可执行的动作。
        :param config: 配置对象，需包含 history_length 等参数。
        """
        # 修改点：使用新的 FullSequenceSearchMemory
        self.memory = FullSequenceSearchMemory()
        super().__init__(envs, projection_f, config)
        
        # 预编译 Summary 提取正则，提升效率
        self._summary_pattern = re.compile(r"<summary>(.*?)</summary>", re.DOTALL | re.IGNORECASE)

    def reset(self, kwargs) -> Tuple[Dict[str, Any], List[Dict]]:
        """
        重置 Batch 环境。
        :return: 包含文本 Prompt、图像观测和中心点信息的字典。
        """
        text_obs, image_obs, infos = self.envs.reset(kwargs=kwargs)

        # 记录每个样本的初始文本状态（中心点基本信息）
        self.initial_states = text_obs
        # 初始化搜索记忆
        self.memory.reset(batch_size=len(text_obs))

        observations = {
            "text": self.build_text_obs(init=True), 
            "image": image_obs, 
            "anchor": text_obs.copy(), 
        }

        return observations, infos

    def step(self, text_actions: List[str]):
        """
        执行模型输出的动作字符串，并更新状态。
        """
        # 1. 提取 Summary
        summaries = []
        for raw_text in text_actions:
            match = self._summary_pattern.search(raw_text)
            if match:
                summaries.append(match.group(1).strip())
            else:
                summaries.append("No summary provided.")

        # 2. 映射动作
        actions, valids = self.projection_f(text_actions)

        # 3. 执行环境交互
        next_text_obs, next_image_obs, rewards, dones, infos = self.envs.step(actions)

        # 4. 存入记忆 (包含 Action, Observation, Summary)
        self.memory.store({
            "search": actions,
            "information": next_text_obs,
            "summary": summaries 
        })

        # 5. 构建下一步观测
        next_observations = {
            "text": self.build_text_obs(init=False), 
            "image": next_image_obs, 
            "anchor": next_text_obs.copy(),
        }

        for i, info in enumerate(infos):
            info["is_action_valid"] = to_numpy(valids[i])

        return next_observations, to_numpy(rewards), to_numpy(dones), infos

    def build_text_obs(self, init: bool) -> List[str]:
        """
        组装最终的文本 Prompt。
        """
        batch_size = len(self.initial_states)
        rendered_prompts: List[str] = []

        # 获取历史上下文
        # 即使 history_length 设置较小（如3），FullSequenceSearchMemory 也会返回完整的 Action/Summary 序列，
        # 仅对 Observation 应用 sliding window 裁剪。
        if not init:
            memory_ctx, _ = self.memory.fetch(
                self.config.env.history_length,
                obs_key="information",
                action_key="search",
                summary_key="summary"
            )
        else:
            memory_ctx = [""] * batch_size

        for i in range(batch_size):
            if init:
                prompt = GRAPH_SEARCH_TEMPLATE_NO_HIS.format(
                    task_instruction=GRAPH_SEARCH_TASK_INSTRUCTION,
                    initial_state=self.initial_states[i],
                )
            else:
                prompt = GRAPH_SEARCH_TEMPLATE_WITH_HIS.format(
                    task_instruction=GRAPH_SEARCH_TASK_INSTRUCTION,
                    initial_state=self.initial_states[i],
                    memory_context=memory_ctx[i],
                    step_count=len(self.memory[i]),
                )
            rendered_prompts.append(prompt)

        return rendered_prompts